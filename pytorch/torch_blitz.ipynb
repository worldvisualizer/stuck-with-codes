{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.2256e+31, 3.2605e-12, 1.8179e+31],\n",
      "        [2.0703e-19, 2.0615e-19, 7.0062e+22],\n",
      "        [3.0304e+35, 1.9338e+34, 5.0782e+31],\n",
      "        [4.2964e+24, 1.7443e+28, 1.0804e+27],\n",
      "        [1.2735e-14, 1.1819e+22, 7.0976e+22]])\n",
      "tensor([[0.3149, 0.4621, 0.0845],\n",
      "        [0.9574, 0.8686, 0.2046],\n",
      "        [0.2491, 0.9609, 0.3284],\n",
      "        [0.0239, 0.6233, 0.5115],\n",
      "        [0.4846, 0.4398, 0.6052]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.5224, -0.4999, -0.7121],\n",
      "        [ 1.6048,  0.7248, -1.7530],\n",
      "        [-1.3934,  1.0264,  1.6323],\n",
      "        [ 1.4303,  1.0740, -2.2985],\n",
      "        [ 0.4919, -1.0918, -0.8380]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 1) tensor: basic matrix of the pytorch\n",
    "\n",
    "# uninitialized tensor: does not contain definite known values\n",
    "x = torch.empty(5,3)\n",
    "print(x)\n",
    "\n",
    "# randomly initialized tensor:\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "\n",
    "# matrix filled zeros and of dtype long tensor:\n",
    "x = torch.zeros(5,3,dtype=torch.long)\n",
    "print(x)\n",
    "\n",
    "# directly from data tensor:\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "\n",
    "# create a tensor out of an existing tensor:\n",
    "x = x.new_ones(5,3, dtype=torch.double) # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "# override dtype, result has same size\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)\n",
    "\n",
    "# get tensor size\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9918, -0.0119,  0.1160],\n",
      "        [ 2.1655,  0.8636, -1.3477],\n",
      "        [-0.5542,  1.8365,  2.3669],\n",
      "        [ 2.3406,  1.6816, -1.3072],\n",
      "        [ 1.3443, -1.0447, -0.6841]])\n",
      "tensor([[ 0.9918, -0.0119,  0.1160],\n",
      "        [ 2.1655,  0.8636, -1.3477],\n",
      "        [-0.5542,  1.8365,  2.3669],\n",
      "        [ 2.3406,  1.6816, -1.3072],\n",
      "        [ 1.3443, -1.0447, -0.6841]])\n",
      "tensor([[ 0.9918, -0.0119,  0.1160],\n",
      "        [ 2.1655,  0.8636, -1.3477],\n",
      "        [-0.5542,  1.8365,  2.3669],\n",
      "        [ 2.3406,  1.6816, -1.3072],\n",
      "        [ 1.3443, -1.0447, -0.6841]])\n",
      "tensor([[ 0.9918, -0.0119,  0.1160],\n",
      "        [ 2.1655,  0.8636, -1.3477],\n",
      "        [-0.5542,  1.8365,  2.3669],\n",
      "        [ 2.3406,  1.6816, -1.3072],\n",
      "        [ 1.3443, -1.0447, -0.6841]])\n",
      "tensor([-0.4999,  0.7248,  1.0264,  1.0740, -1.0918])\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) torch.Size([2, 8])\n",
      "tensor([-1.0428])\n",
      "-1.0428111553192139\n"
     ]
    }
   ],
   "source": [
    "# 2) operations: matrix operations for tensors\n",
    "\n",
    "# addition in several ways\n",
    "y = torch.rand(5,3)\n",
    "print(x + y)\n",
    "print(torch.add(x,y))\n",
    "result = torch.empty(5,3); torch.add(x,y, out=result); print(result) # out-tensor\n",
    "y.add_(x); print(y) # inplace\n",
    "\n",
    "# Any operation that mutates a tensor in-place is post-fixed with an _\n",
    "# For example: x.copy_(y), x.t_(), will change x\n",
    "\n",
    "# numpy like indexing\n",
    "print(x[:, 1]) # all columns, second row\n",
    "\n",
    "# resizing/reshaping: use view\n",
    "a = torch.randn(4,4)\n",
    "b = a.view(16)\n",
    "c = a.view(2,8)\n",
    "d = a.view(-1, 8) # inference based on other dimensions, so column size is inferred\n",
    "print(a.size(), b.size(), c.size(), d.size())\n",
    "\n",
    "# one element tensor: get the value\n",
    "oneelem = torch.randn(1)\n",
    "print(oneelem)\n",
    "print(oneelem.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 3) convert to numpy: torch tensor and numpy array will share their underlying memory locations\n",
    "#                      and changing one will change another\n",
    "\n",
    "a_torch = torch.ones(5)\n",
    "print(a_torch)\n",
    "\n",
    "b_torch = a_torch.numpy()\n",
    "print(b_torch)\n",
    "\n",
    "a_torch.add_(1)\n",
    "print(a_torch)\n",
    "print(b_torch)\n",
    "\n",
    "# converting numpy array to torch tensor\n",
    "# same memory locality\n",
    "import numpy as np\n",
    "aa = np.ones(5)\n",
    "bb = torch.from_numpy(aa)\n",
    "np.add(aa, 1, out=aa)\n",
    "print(aa)\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 4) CUDA tensors: tensors can be moved onto any device using the .to() method\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# :( local laptop does not support CUDA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    cuda_torch = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)\n",
    "    zz = x + cuda_torch\n",
    "    print(zz)\n",
    "    print(zz.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x1131aa5f8>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "False\n",
      "<SumBackward0 object at 0x113087198>\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([863.6843, 495.4630, 740.8290], grad_fn=<MulBackward0>)\n",
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5) Brief introduction to Autograd:\n",
    "\n",
    "autograd package provides automatic differentiation for all operations on tensors. define-by-run framework,\n",
    "so backpropagation is defined by how the code is run, and every iteration can be different\n",
    "\n",
    "by default, autograd tracks all operations on the tensor.\n",
    "when finished with the computation, I can call .backward() and have all gradients computed automatically.\n",
    "this result will be at .grad attribute.\n",
    "this requires memory.\n",
    "\n",
    "stopping: .detach()\n",
    "preventing: with torch.no_grad():\n",
    "\n",
    "Tensor and Function are interconnected and build up an acyclic graph\n",
    "that encodes a complete history of a compuatation - each tensor has a .grad_fn attribute that references\n",
    "a Function that has created the Tensor\n",
    "\n",
    "if you want to compute the derivatives, you can call .backward() on a tensor. if tensor is a scalar,\n",
    "you don't need to specify any arguments to backward(), but if it has more than that,\n",
    "you need to specify a gradient argument that is a tensor of a matching shape.\n",
    "\"\"\"\n",
    "grad_tensor = torch.ones(2,2, requires_grad=True)\n",
    "print(grad_tensor)\n",
    "\n",
    "grad_ops = grad_tensor + 2\n",
    "print(grad_ops)\n",
    "print(grad_ops.grad_fn) # grad fn defined as AddBackward\n",
    "\n",
    "gradgrad = grad_ops * grad_ops * 3\n",
    "out = gradgrad.mean()\n",
    "print(gradgrad, out)\n",
    "\n",
    "a = torch.randn(2,2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn) # grad fn defined as SumBackward\n",
    "\n",
    "out.backward() # equivalent to out.backward(torch.tensor(1.)) because out contains a scalar\n",
    "out # is MeanBackward\n",
    "print(grad_tensor.grad)\n",
    "\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/autograd.html#function\n",
    "\n",
    "\"\"\"\n",
    "Records operation history and defines formulas for differentiating ops.\n",
    "\n",
    "Extending autograd: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\n",
    "\n",
    "Every operation performed on Tensor s creates a new function object,\n",
    "that performs the computation, and records that it happened.\n",
    "The history is retained in the form of a DAG of functions,\n",
    "with edges denoting data dependencies (input <- output).\n",
    "Then, when backward is called, the graph is processed in the topological ordering,\n",
    "by calling backward() methods of each Function object, and passing returned gradients on to next Function s.\n",
    "\n",
    "Normally, the only way users interact with functions is by creating subclasses and defining new operations.\n",
    "This is a recommended way of extending torch.autograd.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6) Neural nets on top of autograd:\n",
    "\n",
    "nn depends on autograd to define models and differentiate them.\n",
    "nn.Module contains layers, and a method forward(input) returns the output\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "1. Define the neural network that has some learnable parameters (or weights)\n",
    "2. Iterate over a dataset of inputs\n",
    "3. Process input through the network\n",
    "4. Compute the loss (how far is the output from being correct)\n",
    "5. Propagate gradients back into the network’s parameters\n",
    "6. Update the weights of the network, typically using a simple update rule:\n",
    "   weight = weight - learning_rate * gradient\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = functional.max_pool2d(functional.relu(self.conv1(x)), (2,2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = functional.max_pool2d(functional.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = functional.relu(self.fc1(x))\n",
    "        x = functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \"\"\"\n",
    "    backward function (where gradients are computed) is automatically defined for you using autograd\n",
    "    You can use any of the Tensor operations in the forward function\n",
    "    \"\"\"\n",
    "    # def backward(self, x):\n",
    "    #     pass\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# learnable parameters of a model are returned by net.parameters()\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "\n",
    "input_vector = torch.randn(1,1,32,32)\n",
    "# nn only supports mini-batches, not a single sample. so\n",
    "# nn.Conv2d takes 4d vector of nSamples x nChannels x Height x Width\n",
    "out = net(input_vector)\n",
    "print(out)\n",
    "\n",
    "# zero the gradient buffers of all parameters and backprops with random gradients\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8506, grad_fn=<MseLossBackward>)\n",
      "<MseLossBackward object at 0x1131b8ac8>\n",
      "<AddmmBackward object at 0x1131aa7f0>\n",
      "<AccumulateGrad object at 0x1131b8ac8>\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "output = net(input_vector)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "print(loss.grad_fn) # MSE loss\n",
    "print(loss.grad_fn.next_functions[0][0]) # linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0012, -0.0011,  0.0016,  0.0094,  0.0061,  0.0110])\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the weights\n",
    "\n",
    "# this is the simple python code for gradient descent\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "    \n",
    "import torch.optim as optim\n",
    "\n",
    "# optimizer (update rules) - SGD, nesterov-SGD, Adam, RMSProp, ...\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# in the training loop:\n",
    "optimizer.zero_grad()\n",
    "output = net(input_vector)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step() # update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 1, 3, 3])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 3, 3])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 576])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.0766,  0.1466, -0.2599],\n",
       "                        [ 0.1632,  0.0884,  0.1685],\n",
       "                        [-0.2542,  0.2027, -0.0301]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1326, -0.0435,  0.2925],\n",
       "                        [-0.0614, -0.2994, -0.3049],\n",
       "                        [-0.2435, -0.2369, -0.2051]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2613,  0.2208,  0.3055],\n",
       "                        [-0.2224, -0.1650, -0.0793],\n",
       "                        [-0.2238,  0.2733, -0.2582]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0802,  0.2403,  0.0256],\n",
       "                        [-0.2966,  0.0563,  0.2104],\n",
       "                        [ 0.1387, -0.2813,  0.0400]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2897, -0.1730,  0.0207],\n",
       "                        [ 0.1945,  0.1883,  0.1839],\n",
       "                        [ 0.2531,  0.1738, -0.2217]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2337, -0.1315, -0.2089],\n",
       "                        [ 0.1859,  0.3034,  0.2066],\n",
       "                        [ 0.1168, -0.0114,  0.0038]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.2547,  0.1597,  0.2401,  0.2604,  0.1262, -0.2992])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[-8.2742e-03, -1.0260e-01,  1.2672e-01],\n",
       "                        [-6.4975e-02,  8.4360e-02,  8.4465e-02],\n",
       "                        [-2.5703e-02,  1.0038e-01,  8.7997e-02]],\n",
       "              \n",
       "                       [[ 4.3055e-02,  1.0236e-01, -5.2678e-02],\n",
       "                        [ 1.1313e-01, -8.6902e-02, -1.2621e-01],\n",
       "                        [ 5.3422e-02,  7.9280e-02,  4.4847e-02]],\n",
       "              \n",
       "                       [[ 2.6548e-02,  5.8055e-02,  1.2412e-01],\n",
       "                        [ 5.0107e-02, -6.9916e-02,  9.9225e-02],\n",
       "                        [-2.7150e-02,  8.3149e-02,  1.4044e-02]],\n",
       "              \n",
       "                       [[ 7.8839e-02,  8.8434e-02,  1.1217e-01],\n",
       "                        [-1.0610e-01,  6.4690e-02,  7.6673e-03],\n",
       "                        [-1.4667e-02,  1.0939e-02,  1.1269e-01]],\n",
       "              \n",
       "                       [[ 5.5196e-02,  8.1358e-02, -1.2054e-01],\n",
       "                        [ 3.0259e-02, -1.0510e-02,  7.8147e-02],\n",
       "                        [-1.1288e-01, -1.0709e-01, -6.3763e-02]],\n",
       "              \n",
       "                       [[-2.7457e-03, -7.7635e-02, -9.2300e-02],\n",
       "                        [-2.1530e-02,  1.0392e-01,  9.1894e-02],\n",
       "                        [ 2.2661e-02, -6.8888e-02,  1.2467e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0546e-02,  3.4930e-02, -1.1666e-01],\n",
       "                        [ 3.8363e-02,  3.7429e-02, -8.2811e-02],\n",
       "                        [-1.1941e-01, -3.7675e-02,  1.0933e-01]],\n",
       "              \n",
       "                       [[-1.0838e-01, -2.8762e-02,  2.4725e-02],\n",
       "                        [-3.5359e-02, -7.1556e-02,  1.3407e-01],\n",
       "                        [-7.5913e-02, -1.3113e-01,  1.0276e-01]],\n",
       "              \n",
       "                       [[-1.8246e-02,  2.8394e-02,  8.3433e-02],\n",
       "                        [-7.8149e-02, -9.9840e-02,  1.2570e-01],\n",
       "                        [ 7.0870e-02, -1.1311e-01,  7.8866e-02]],\n",
       "              \n",
       "                       [[ 1.3432e-01,  6.9706e-02, -9.7155e-03],\n",
       "                        [ 8.1290e-02,  1.6180e-02, -1.2416e-01],\n",
       "                        [ 3.5771e-02,  3.6263e-02, -1.3003e-01]],\n",
       "              \n",
       "                       [[-4.4752e-02,  1.2792e-01,  9.9751e-02],\n",
       "                        [-9.8767e-02, -5.6033e-02, -8.3787e-02],\n",
       "                        [-1.2232e-01,  2.9758e-02,  9.9333e-02]],\n",
       "              \n",
       "                       [[ 1.0225e-01,  8.1663e-02, -6.7566e-02],\n",
       "                        [ 1.0471e-01,  8.2042e-02,  3.9910e-02],\n",
       "                        [ 1.0863e-01,  9.0959e-02, -1.1912e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-9.3664e-02,  4.2743e-02,  1.1927e-01],\n",
       "                        [-5.4773e-02,  1.8361e-02,  1.0807e-01],\n",
       "                        [-6.7336e-02,  9.0967e-02, -1.1685e-01]],\n",
       "              \n",
       "                       [[ 1.3103e-01,  6.2130e-02, -3.0199e-02],\n",
       "                        [ 1.3135e-01, -5.5092e-02, -1.1684e-01],\n",
       "                        [-6.7558e-02, -3.2495e-02, -1.2928e-01]],\n",
       "              \n",
       "                       [[ 3.8712e-02, -6.7249e-02, -4.4669e-02],\n",
       "                        [ 2.5939e-02, -1.0197e-01,  8.5425e-03],\n",
       "                        [-1.1890e-01,  1.2096e-01,  1.1672e-01]],\n",
       "              \n",
       "                       [[ 7.4534e-02,  8.6851e-02,  3.0654e-02],\n",
       "                        [ 6.2799e-02,  5.4990e-02, -7.8381e-02],\n",
       "                        [ 4.2253e-03, -2.3428e-02,  9.0475e-02]],\n",
       "              \n",
       "                       [[ 4.2119e-02, -1.3121e-01, -1.1444e-01],\n",
       "                        [-1.2243e-01, -1.0813e-01,  1.1500e-01],\n",
       "                        [ 5.9907e-02, -3.0636e-02, -8.7678e-02]],\n",
       "              \n",
       "                       [[ 5.4919e-02, -5.0296e-02, -8.3869e-02],\n",
       "                        [ 4.9519e-02, -1.0164e-01,  3.1941e-02],\n",
       "                        [-1.0358e-01,  4.4654e-02,  7.6975e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.3048e-01,  1.2615e-01, -3.2546e-02],\n",
       "                        [ 9.4010e-02,  1.6865e-02, -2.2000e-02],\n",
       "                        [-9.0139e-02, -2.1711e-02,  5.2791e-02]],\n",
       "              \n",
       "                       [[-2.7269e-02,  5.4505e-02, -1.1474e-01],\n",
       "                        [ 1.3757e-02,  3.9201e-02,  3.3960e-02],\n",
       "                        [-1.2968e-01, -1.0345e-01,  3.3643e-03]],\n",
       "              \n",
       "                       [[-7.9699e-02,  2.4548e-02, -1.2691e-01],\n",
       "                        [ 7.8612e-03, -7.5287e-02, -2.0327e-02],\n",
       "                        [ 2.1785e-02,  4.6072e-02,  2.6968e-02]],\n",
       "              \n",
       "                       [[-2.6551e-02, -6.4240e-02,  1.0266e-01],\n",
       "                        [ 5.0644e-02,  2.0812e-02, -2.3639e-02],\n",
       "                        [-9.9229e-02,  5.8073e-02,  4.3115e-02]],\n",
       "              \n",
       "                       [[ 8.8647e-02, -1.0889e-01,  5.2555e-03],\n",
       "                        [ 1.1739e-01, -9.3254e-02, -1.3032e-01],\n",
       "                        [ 3.7839e-02,  2.1114e-02,  1.3552e-01]],\n",
       "              \n",
       "                       [[ 7.0438e-02, -1.3192e-01, -1.7943e-02],\n",
       "                        [-1.0494e-01, -1.2181e-01, -8.0868e-02],\n",
       "                        [ 4.7232e-02, -8.8546e-03,  1.1998e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.2992e-02, -8.1137e-02,  1.2360e-02],\n",
       "                        [-7.3581e-02, -8.1300e-03, -1.2029e-01],\n",
       "                        [-3.0420e-02, -1.3007e-01,  9.2524e-02]],\n",
       "              \n",
       "                       [[-8.0528e-02, -7.9502e-02, -9.8450e-02],\n",
       "                        [-1.0502e-01, -7.0310e-02,  8.8161e-03],\n",
       "                        [ 6.6370e-02, -1.7236e-02, -6.5199e-02]],\n",
       "              \n",
       "                       [[-5.0214e-02, -6.0987e-02, -5.9288e-02],\n",
       "                        [ 1.0407e-01,  9.8353e-02,  2.0872e-02],\n",
       "                        [ 5.1685e-02, -9.0337e-03, -6.7895e-02]],\n",
       "              \n",
       "                       [[ 8.3913e-02, -6.6391e-02,  9.5959e-02],\n",
       "                        [-1.1907e-01, -1.0315e-01, -9.5611e-02],\n",
       "                        [-6.8559e-02,  9.6864e-02, -3.5123e-02]],\n",
       "              \n",
       "                       [[-5.7857e-02,  1.9821e-02, -1.3320e-01],\n",
       "                        [ 1.2544e-01,  2.9987e-02,  8.2276e-02],\n",
       "                        [ 5.2425e-02,  2.0374e-05, -2.9598e-02]],\n",
       "              \n",
       "                       [[ 4.8550e-02, -5.1708e-03,  4.3194e-02],\n",
       "                        [-9.0232e-02,  8.1957e-02,  1.2841e-01],\n",
       "                        [ 7.8705e-02,  1.1335e-01, -1.2235e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.6278e-02,  1.0262e-01,  1.2782e-01],\n",
       "                        [-1.3486e-01, -1.0773e-01,  2.5041e-02],\n",
       "                        [ 6.4561e-02,  3.1236e-02,  1.7828e-02]],\n",
       "              \n",
       "                       [[-1.2656e-01,  1.6826e-02, -9.1523e-02],\n",
       "                        [ 6.5214e-02,  1.2547e-01, -5.4194e-02],\n",
       "                        [-6.4879e-02,  1.9496e-02,  4.9277e-02]],\n",
       "              \n",
       "                       [[ 8.4726e-02,  1.1746e-01,  5.4692e-02],\n",
       "                        [ 7.5367e-04,  7.9858e-02, -1.3243e-01],\n",
       "                        [-1.1087e-01,  6.8377e-02,  8.2870e-02]],\n",
       "              \n",
       "                       [[ 1.1253e-01,  2.1622e-02, -1.0033e-01],\n",
       "                        [ 2.5423e-02,  1.8911e-03, -1.2815e-01],\n",
       "                        [ 4.8011e-02,  3.2978e-02, -1.2650e-01]],\n",
       "              \n",
       "                       [[-1.9019e-02, -1.2348e-01,  1.0739e-01],\n",
       "                        [-4.7246e-03,  1.8524e-03,  9.2119e-02],\n",
       "                        [-4.8797e-02,  6.8752e-02, -6.6355e-02]],\n",
       "              \n",
       "                       [[-7.4894e-02,  1.9139e-02, -1.2844e-01],\n",
       "                        [-9.8234e-02, -1.2997e-01, -1.8670e-03],\n",
       "                        [ 3.2453e-02,  4.7814e-02,  7.8009e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7345e-02, -2.3273e-02, -2.0750e-02],\n",
       "                        [-2.8453e-02,  6.2580e-02, -3.5825e-02],\n",
       "                        [ 8.1249e-02,  2.2097e-02, -9.5724e-02]],\n",
       "              \n",
       "                       [[-5.1081e-02,  1.3319e-01, -1.0977e-01],\n",
       "                        [ 8.9563e-02,  3.9245e-02, -7.0349e-02],\n",
       "                        [ 8.9498e-02, -5.7719e-02, -7.2667e-02]],\n",
       "              \n",
       "                       [[ 1.1196e-01,  6.1227e-02,  1.0974e-02],\n",
       "                        [-8.3078e-02,  9.8585e-02,  1.3559e-01],\n",
       "                        [-1.0242e-01,  1.9158e-02,  8.5889e-02]],\n",
       "              \n",
       "                       [[ 1.2479e-01, -1.2056e-02,  7.8644e-02],\n",
       "                        [ 2.1729e-02,  3.7187e-02,  1.1701e-01],\n",
       "                        [-6.0439e-02, -2.1893e-02,  9.3476e-02]],\n",
       "              \n",
       "                       [[ 2.4520e-02,  1.1563e-01,  5.3707e-02],\n",
       "                        [ 8.6040e-02,  5.5867e-02,  5.6220e-02],\n",
       "                        [-1.2457e-02,  1.0685e-01, -1.1866e-01]],\n",
       "              \n",
       "                       [[ 9.0904e-02, -8.1535e-02, -5.6627e-02],\n",
       "                        [ 5.4309e-02,  9.4076e-02, -6.8852e-02],\n",
       "                        [ 7.0670e-02,  5.6979e-02,  2.0336e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9514e-02,  1.3145e-01,  5.9260e-02],\n",
       "                        [-9.3548e-03,  8.3929e-02,  2.2152e-02],\n",
       "                        [ 5.8198e-02,  1.2845e-01,  8.5166e-03]],\n",
       "              \n",
       "                       [[-6.5810e-02, -4.1331e-02,  5.8607e-02],\n",
       "                        [-1.0221e-01, -1.1969e-01, -7.9912e-02],\n",
       "                        [-2.8473e-02, -1.2398e-02,  2.9062e-02]],\n",
       "              \n",
       "                       [[-3.6903e-02,  7.4581e-02, -1.1921e-01],\n",
       "                        [-1.7598e-02,  7.7846e-03,  5.4348e-02],\n",
       "                        [-2.1406e-02, -1.0634e-01, -1.4571e-02]],\n",
       "              \n",
       "                       [[-1.0982e-01,  9.1311e-02, -4.7346e-02],\n",
       "                        [-3.9025e-02, -2.8779e-02, -3.8066e-02],\n",
       "                        [-1.1509e-01,  1.2595e-01, -5.6306e-02]],\n",
       "              \n",
       "                       [[-1.0167e-01, -5.3802e-02,  1.0010e-01],\n",
       "                        [-3.2507e-02, -7.5598e-02,  6.6778e-02],\n",
       "                        [ 9.8757e-02,  9.3548e-02, -1.3265e-01]],\n",
       "              \n",
       "                       [[-3.0377e-02, -5.0703e-02,  5.0805e-02],\n",
       "                        [ 5.9084e-02, -1.7267e-03,  2.2615e-02],\n",
       "                        [-2.4010e-02,  5.2746e-02, -2.2765e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.6104e-02, -2.6280e-02,  5.2194e-02],\n",
       "                        [ 3.5505e-02, -1.5674e-02, -6.1159e-02],\n",
       "                        [ 7.5717e-02,  4.4614e-02,  1.0448e-01]],\n",
       "              \n",
       "                       [[ 1.4049e-02, -1.0237e-01, -8.8768e-02],\n",
       "                        [ 1.2384e-01,  6.3337e-02, -5.9583e-02],\n",
       "                        [ 6.2035e-02, -8.9590e-02,  1.4813e-02]],\n",
       "              \n",
       "                       [[-9.2883e-02, -6.5307e-02, -9.2517e-02],\n",
       "                        [-6.9904e-02, -8.7893e-02, -3.2105e-02],\n",
       "                        [-6.9082e-02,  2.4419e-02, -1.0588e-01]],\n",
       "              \n",
       "                       [[ 2.5353e-02,  4.8836e-02,  7.7552e-02],\n",
       "                        [ 1.5666e-02, -1.1293e-01,  1.2260e-01],\n",
       "                        [ 1.2487e-02,  6.1447e-02, -5.4030e-02]],\n",
       "              \n",
       "                       [[ 9.0899e-02, -5.4315e-02, -2.2613e-03],\n",
       "                        [-2.7527e-02,  8.3595e-02, -6.7247e-02],\n",
       "                        [-2.7908e-02, -4.2185e-02, -6.0694e-03]],\n",
       "              \n",
       "                       [[-6.3937e-02, -1.6007e-02,  1.2681e-02],\n",
       "                        [ 2.8491e-02,  1.0186e-01, -6.2952e-02],\n",
       "                        [ 1.8589e-02,  9.0638e-02, -6.2643e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.2895e-02, -1.4603e-02,  3.8002e-02],\n",
       "                        [ 9.5395e-02,  9.0974e-02, -9.0056e-02],\n",
       "                        [-1.2085e-01,  3.3784e-02, -4.0603e-03]],\n",
       "              \n",
       "                       [[ 1.4787e-02, -5.4265e-03, -5.0202e-02],\n",
       "                        [ 1.1459e-01, -1.3474e-01,  6.9433e-02],\n",
       "                        [ 1.1832e-01,  8.8505e-02, -9.6047e-02]],\n",
       "              \n",
       "                       [[ 7.9247e-03,  6.5829e-02, -4.6916e-02],\n",
       "                        [ 4.0430e-03, -4.4864e-02,  3.2604e-02],\n",
       "                        [ 2.5899e-02, -4.5501e-02,  1.0048e-01]],\n",
       "              \n",
       "                       [[-6.6485e-02, -4.3961e-02, -5.0604e-02],\n",
       "                        [-1.2661e-02,  6.7220e-02,  9.8021e-02],\n",
       "                        [ 1.7819e-02,  4.0081e-02,  6.4380e-02]],\n",
       "              \n",
       "                       [[-7.8784e-02,  7.9232e-02, -9.7140e-02],\n",
       "                        [-1.0710e-02,  1.0090e-01, -7.6028e-03],\n",
       "                        [-4.5950e-02, -5.6376e-02,  1.6134e-02]],\n",
       "              \n",
       "                       [[ 1.2250e-01,  1.2625e-02,  6.1340e-02],\n",
       "                        [-1.0029e-02,  5.5610e-03,  5.1675e-02],\n",
       "                        [-3.5194e-02,  6.4514e-03,  1.3141e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.5901e-02, -7.9150e-02,  4.3743e-03],\n",
       "                        [-1.1952e-01, -5.7648e-02,  7.4486e-02],\n",
       "                        [ 6.4303e-02,  7.9715e-02,  3.4692e-02]],\n",
       "              \n",
       "                       [[-4.7154e-02, -1.3890e-02,  1.3461e-01],\n",
       "                        [ 1.0007e-01,  1.2964e-01,  1.5240e-02],\n",
       "                        [-1.3607e-02,  8.2592e-02, -1.2891e-01]],\n",
       "              \n",
       "                       [[ 5.4251e-02,  1.0467e-01, -1.3351e-01],\n",
       "                        [-7.8218e-02, -9.3098e-02, -8.2713e-02],\n",
       "                        [-3.0052e-02,  8.8507e-02,  7.1586e-02]],\n",
       "              \n",
       "                       [[-7.7263e-02, -5.3633e-03,  4.6509e-02],\n",
       "                        [ 1.1429e-01,  1.0758e-01,  1.2349e-01],\n",
       "                        [ 6.9456e-02, -7.0833e-02,  7.3794e-02]],\n",
       "              \n",
       "                       [[ 1.1339e-01,  1.1189e-01, -1.0018e-01],\n",
       "                        [-5.7532e-02, -5.7182e-02,  2.4271e-04],\n",
       "                        [-1.1214e-01,  8.5838e-02,  5.2993e-02]],\n",
       "              \n",
       "                       [[-1.0614e-01, -2.1915e-02,  3.2600e-02],\n",
       "                        [ 8.9912e-02,  5.1102e-02, -1.2718e-02],\n",
       "                        [ 6.0010e-02,  1.0754e-01,  2.1499e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.6192e-03,  3.0521e-03,  1.2541e-01],\n",
       "                        [-2.1774e-02,  1.1661e-01, -3.8299e-02],\n",
       "                        [ 8.7313e-02,  1.3441e-01,  5.3410e-02]],\n",
       "              \n",
       "                       [[-1.1812e-01,  5.8584e-02, -7.1597e-02],\n",
       "                        [ 1.2000e-01,  7.3065e-02, -8.2746e-02],\n",
       "                        [ 9.6481e-02,  3.2057e-02,  1.0669e-02]],\n",
       "              \n",
       "                       [[ 7.8591e-02,  4.2677e-02, -7.3306e-02],\n",
       "                        [-4.1716e-02,  9.0691e-02, -4.1053e-02],\n",
       "                        [-2.3595e-02,  1.2396e-01,  1.1648e-01]],\n",
       "              \n",
       "                       [[ 1.1395e-01,  5.3215e-02, -7.1937e-02],\n",
       "                        [-8.9763e-02, -1.0709e-01,  1.0706e-01],\n",
       "                        [-1.2677e-01, -4.0674e-02, -7.1225e-03]],\n",
       "              \n",
       "                       [[-1.1263e-01,  6.3002e-02,  1.2421e-03],\n",
       "                        [ 8.3219e-02, -9.1852e-02, -1.1964e-01],\n",
       "                        [ 2.3816e-02,  6.8217e-02, -2.1344e-02]],\n",
       "              \n",
       "                       [[-1.1634e-01,  5.0718e-02,  1.5117e-02],\n",
       "                        [ 6.3076e-02,  1.2585e-01,  6.5245e-02],\n",
       "                        [ 6.0365e-02,  2.7521e-02,  1.0353e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2135e-01,  4.3264e-02,  1.5781e-02],\n",
       "                        [-6.1785e-02, -5.5793e-02,  1.2852e-01],\n",
       "                        [-4.6458e-02, -6.0277e-02,  2.8757e-02]],\n",
       "              \n",
       "                       [[-9.7052e-02,  9.5710e-02,  4.1355e-04],\n",
       "                        [-1.1115e-01,  2.1063e-02, -2.1417e-02],\n",
       "                        [-3.5033e-02,  3.1817e-02, -1.1670e-01]],\n",
       "              \n",
       "                       [[ 6.2725e-02,  2.9380e-02,  3.6360e-02],\n",
       "                        [-8.8230e-02, -5.8906e-02, -1.3567e-01],\n",
       "                        [ 9.6845e-02, -8.0379e-03,  1.0911e-02]],\n",
       "              \n",
       "                       [[-8.4793e-02, -1.0677e-01,  4.8541e-02],\n",
       "                        [-4.7344e-02,  6.8611e-02, -1.1012e-01],\n",
       "                        [ 8.1789e-02, -7.5600e-02, -3.4556e-02]],\n",
       "              \n",
       "                       [[ 1.0676e-01,  7.9038e-02, -8.5234e-03],\n",
       "                        [ 1.0288e-01,  8.1561e-02, -1.2134e-01],\n",
       "                        [ 1.2036e-01, -1.0699e-02, -1.9228e-02]],\n",
       "              \n",
       "                       [[ 1.4344e-04, -3.7620e-02,  1.2726e-01],\n",
       "                        [ 2.8832e-02, -1.2735e-01, -1.3397e-01],\n",
       "                        [-3.6672e-03,  1.1802e-01, -7.5434e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.0555e-02, -7.4652e-02,  6.2961e-02],\n",
       "                        [ 9.9729e-02, -8.3237e-02, -8.2153e-02],\n",
       "                        [-1.0020e-01,  1.3578e-01, -3.2666e-02]],\n",
       "              \n",
       "                       [[ 6.2331e-02, -6.2172e-02,  3.1984e-02],\n",
       "                        [-7.3458e-02, -9.7869e-02,  7.4863e-02],\n",
       "                        [-1.0084e-01,  3.4165e-02,  6.2415e-02]],\n",
       "              \n",
       "                       [[-9.3419e-02, -6.4296e-02,  1.3344e-01],\n",
       "                        [-5.2773e-02,  2.6182e-02,  9.4395e-02],\n",
       "                        [ 5.1812e-02, -5.3693e-02,  1.0792e-01]],\n",
       "              \n",
       "                       [[-7.5853e-02,  1.2722e-01, -4.7820e-02],\n",
       "                        [ 9.5749e-02,  7.4118e-03,  2.2574e-03],\n",
       "                        [ 1.1474e-01,  1.0164e-02, -1.2959e-01]],\n",
       "              \n",
       "                       [[-2.0861e-02,  4.3244e-02,  3.8604e-02],\n",
       "                        [-9.7207e-02,  4.4682e-04, -1.3561e-03],\n",
       "                        [-1.1085e-02,  7.3353e-03,  1.0329e-01]],\n",
       "              \n",
       "                       [[ 7.4536e-02,  4.6551e-02, -1.0689e-01],\n",
       "                        [-7.3862e-02,  3.3942e-03, -1.0837e-01],\n",
       "                        [ 2.5170e-02, -1.7393e-03, -7.4657e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.5530e-02, -7.4807e-03,  3.5483e-02],\n",
       "                        [ 2.6499e-02, -7.4839e-02,  1.0063e-01],\n",
       "                        [ 4.4043e-02,  8.9343e-02,  8.9538e-02]],\n",
       "              \n",
       "                       [[ 9.6660e-02, -5.0095e-02,  1.2769e-01],\n",
       "                        [ 8.6236e-02, -1.0349e-01, -1.6228e-02],\n",
       "                        [ 9.8196e-02,  7.3980e-03, -6.3693e-02]],\n",
       "              \n",
       "                       [[ 8.9836e-02,  5.6718e-02,  2.7221e-02],\n",
       "                        [ 1.2129e-01,  9.7594e-02, -3.8980e-02],\n",
       "                        [ 7.9501e-02, -3.2409e-02,  2.0548e-02]],\n",
       "              \n",
       "                       [[ 1.0129e-01,  9.5379e-02, -9.1039e-02],\n",
       "                        [ 3.3804e-02, -7.6102e-02, -7.9250e-02],\n",
       "                        [-2.0850e-02, -1.0739e-01,  2.3086e-02]],\n",
       "              \n",
       "                       [[ 9.5925e-03, -7.2395e-02, -1.2270e-01],\n",
       "                        [-7.9422e-02,  4.8689e-02, -8.4157e-02],\n",
       "                        [ 1.7494e-02, -2.2291e-03,  5.5018e-02]],\n",
       "              \n",
       "                       [[ 4.1550e-02,  9.3982e-03, -2.7255e-02],\n",
       "                        [ 9.1922e-02, -3.5271e-02,  1.2839e-01],\n",
       "                        [ 9.3302e-02, -1.1567e-01, -1.3271e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.7999e-02, -1.2377e-01, -1.0044e-01],\n",
       "                        [ 1.2136e-01,  7.9030e-02,  1.3444e-02],\n",
       "                        [-1.1931e-01,  4.1905e-02,  5.1856e-02]],\n",
       "              \n",
       "                       [[-9.6665e-02, -1.3186e-01, -7.1421e-02],\n",
       "                        [ 1.4934e-02, -1.2184e-01, -6.3541e-02],\n",
       "                        [-9.0261e-02, -1.0712e-01, -1.7392e-02]],\n",
       "              \n",
       "                       [[-1.3424e-01, -1.0561e-01, -1.5144e-02],\n",
       "                        [-8.5547e-02, -8.9288e-02,  7.8421e-02],\n",
       "                        [ 1.2816e-01,  1.4785e-02,  5.8378e-03]],\n",
       "              \n",
       "                       [[ 4.1798e-02,  7.1782e-02,  2.8161e-02],\n",
       "                        [-1.3073e-01, -6.4344e-02, -6.5257e-02],\n",
       "                        [ 5.6889e-03,  1.1970e-01, -1.2924e-01]],\n",
       "              \n",
       "                       [[-8.2475e-02,  4.2687e-02,  1.2812e-01],\n",
       "                        [ 2.3025e-03,  5.0587e-02, -6.0788e-02],\n",
       "                        [ 5.7092e-02,  4.2144e-02,  2.2930e-02]],\n",
       "              \n",
       "                       [[-1.8638e-02,  7.5111e-02, -2.6940e-02],\n",
       "                        [-1.2608e-01, -6.2902e-02,  1.5180e-02],\n",
       "                        [-1.5557e-02,  1.3446e-01, -1.0098e-01]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([-0.1260,  0.0549, -0.0940, -0.0584, -0.0839, -0.1081,  0.1343, -0.0282,\n",
       "                       0.1082, -0.1031, -0.0661,  0.0747,  0.0019, -0.0907, -0.0731, -0.0436])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0238,  0.0314,  0.0124,  ...,  0.0130, -0.0058, -0.0029],\n",
       "                      [ 0.0134, -0.0085, -0.0404,  ...,  0.0133, -0.0312, -0.0072],\n",
       "                      [ 0.0082, -0.0142, -0.0386,  ...,  0.0159,  0.0122,  0.0292],\n",
       "                      ...,\n",
       "                      [-0.0315, -0.0008,  0.0101,  ..., -0.0166,  0.0385, -0.0292],\n",
       "                      [ 0.0200, -0.0397,  0.0309,  ..., -0.0371, -0.0256, -0.0230],\n",
       "                      [-0.0087,  0.0219,  0.0335,  ..., -0.0381, -0.0090,  0.0381]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0200,  0.0379,  0.0396, -0.0019,  0.0408, -0.0242,  0.0080,  0.0298,\n",
       "                       0.0375,  0.0070, -0.0303, -0.0195,  0.0239,  0.0085,  0.0297,  0.0264,\n",
       "                      -0.0189,  0.0308,  0.0176,  0.0229,  0.0257, -0.0254, -0.0011,  0.0180,\n",
       "                       0.0170, -0.0299, -0.0198, -0.0221,  0.0219,  0.0047,  0.0071,  0.0343,\n",
       "                       0.0399,  0.0390,  0.0135,  0.0277,  0.0343,  0.0096,  0.0143,  0.0181,\n",
       "                       0.0037, -0.0350,  0.0002, -0.0333, -0.0253, -0.0156,  0.0221,  0.0131,\n",
       "                      -0.0032, -0.0352,  0.0383, -0.0211,  0.0013, -0.0320,  0.0113,  0.0186,\n",
       "                       0.0081, -0.0221, -0.0084, -0.0328,  0.0056,  0.0045,  0.0373, -0.0290,\n",
       "                      -0.0178,  0.0064,  0.0236,  0.0096,  0.0312, -0.0203, -0.0372, -0.0159,\n",
       "                       0.0082, -0.0286, -0.0117,  0.0236,  0.0377, -0.0218, -0.0156,  0.0034,\n",
       "                       0.0076, -0.0199,  0.0245, -0.0003,  0.0306, -0.0404,  0.0239,  0.0120,\n",
       "                      -0.0169, -0.0114, -0.0211,  0.0191,  0.0126,  0.0372,  0.0357,  0.0202,\n",
       "                       0.0222, -0.0037,  0.0006,  0.0356, -0.0257, -0.0059, -0.0156, -0.0144,\n",
       "                      -0.0333, -0.0028,  0.0193, -0.0269, -0.0387, -0.0179, -0.0096, -0.0412,\n",
       "                       0.0123, -0.0139, -0.0217,  0.0411,  0.0014,  0.0365, -0.0391, -0.0319])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0120,  0.0377,  0.0033,  ...,  0.0824,  0.0321,  0.0334],\n",
       "                      [-0.0747, -0.0593,  0.0693,  ...,  0.0041, -0.0837, -0.0222],\n",
       "                      [ 0.0861, -0.0463, -0.0833,  ...,  0.0750,  0.0385, -0.0143],\n",
       "                      ...,\n",
       "                      [ 0.0296,  0.0439, -0.0710,  ..., -0.0412, -0.0878,  0.0514],\n",
       "                      [ 0.0342,  0.0323, -0.0334,  ..., -0.0868, -0.0103, -0.0058],\n",
       "                      [-0.0369,  0.0628, -0.0625,  ...,  0.0282, -0.0683, -0.0898]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0259,  0.0465,  0.0081, -0.0623, -0.0377, -0.0012,  0.0586,  0.0577,\n",
       "                      -0.0596, -0.0755, -0.0465,  0.0166,  0.0095, -0.0038,  0.0260, -0.0137,\n",
       "                       0.0064, -0.0447, -0.0540, -0.0121, -0.0016,  0.0161,  0.0562,  0.0216,\n",
       "                       0.0124, -0.0801,  0.0206,  0.0869, -0.0739,  0.0094, -0.0519, -0.0030,\n",
       "                      -0.0013, -0.0781,  0.0538, -0.0557,  0.0250, -0.0512,  0.0139,  0.0393,\n",
       "                       0.0302, -0.0330, -0.0709,  0.0636,  0.0667,  0.0453,  0.0094, -0.0704,\n",
       "                      -0.0476, -0.0648,  0.0591,  0.0141, -0.0091,  0.0766, -0.0065,  0.0578,\n",
       "                       0.0541,  0.0639,  0.0348, -0.0912, -0.0077,  0.0182, -0.0366, -0.0739,\n",
       "                      -0.0811,  0.0366,  0.0069, -0.0883,  0.0629,  0.0293, -0.0063,  0.0533,\n",
       "                      -0.0854,  0.0796, -0.0319, -0.0541, -0.0302,  0.0183,  0.0225,  0.0875,\n",
       "                       0.0252,  0.0167,  0.0435,  0.0757])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 0.0066,  0.0945,  0.0156,  0.0717, -0.1020, -0.0553,  0.0259, -0.0026,\n",
       "                       -0.0164, -0.0830,  0.0015, -0.0839,  0.0530,  0.0161,  0.0035, -0.0334,\n",
       "                       -0.0689,  0.0384, -0.0898,  0.0252,  0.0493, -0.0879, -0.0933,  0.0836,\n",
       "                       -0.0433, -0.0545,  0.0835, -0.0699,  0.0528,  0.0670,  0.0082, -0.0584,\n",
       "                       -0.0222, -0.0675, -0.0033, -0.0058, -0.0493, -0.0870, -0.0672,  0.0663,\n",
       "                        0.0981, -0.0345,  0.0294,  0.0017,  0.1000,  0.0301, -0.0363,  0.0468,\n",
       "                       -0.0544,  0.0322,  0.0058, -0.0103,  0.0049,  0.0242, -0.0072, -0.0811,\n",
       "                       -0.0208, -0.0469, -0.0917,  0.0172, -0.0268, -0.0483,  0.0486,  0.0582,\n",
       "                        0.0263, -0.0283, -0.0699,  0.0357,  0.0533, -0.0879,  0.0040,  0.1037,\n",
       "                       -0.0270,  0.0506,  0.0507,  0.0894,  0.0141, -0.1035, -0.0693,  0.0308,\n",
       "                       -0.0755, -0.0368,  0.0305,  0.0480],\n",
       "                      [-0.0073, -0.0507,  0.0082, -0.0599, -0.0993, -0.0027, -0.0456, -0.0518,\n",
       "                       -0.0289, -0.0532, -0.0511,  0.0722,  0.0329,  0.0150, -0.0330, -0.0469,\n",
       "                       -0.0829, -0.0723, -0.0161,  0.0130, -0.0669, -0.0827, -0.0599,  0.0911,\n",
       "                        0.0671, -0.0987, -0.0599, -0.0356,  0.0087,  0.0869,  0.1075, -0.1007,\n",
       "                        0.0712,  0.0777,  0.0148,  0.0080, -0.0922, -0.0226, -0.0418,  0.0421,\n",
       "                        0.0024, -0.0072,  0.0068, -0.1003,  0.0395,  0.0296,  0.0714,  0.0842,\n",
       "                        0.0739, -0.0413,  0.0587,  0.0847,  0.0526, -0.0193, -0.0931, -0.0042,\n",
       "                       -0.0979,  0.0488,  0.0027, -0.0939,  0.0492,  0.0609,  0.0749,  0.0963,\n",
       "                        0.0526, -0.0627, -0.0724,  0.0677,  0.0546, -0.1015,  0.0110, -0.0811,\n",
       "                        0.0786, -0.1026, -0.0194,  0.0364, -0.0034,  0.0693, -0.0996, -0.0734,\n",
       "                        0.0309, -0.0309, -0.0050, -0.0225],\n",
       "                      [-0.0914,  0.0451, -0.0102, -0.0619, -0.0265, -0.0519,  0.0989,  0.0846,\n",
       "                        0.0124, -0.0685,  0.0095, -0.0284, -0.1037,  0.0966, -0.0222,  0.0097,\n",
       "                        0.0899, -0.0565, -0.0441,  0.0900, -0.0489,  0.0694,  0.0825, -0.0905,\n",
       "                       -0.0678, -0.0870, -0.0073, -0.0261, -0.0256, -0.0663,  0.0476, -0.0819,\n",
       "                       -0.0899, -0.0309,  0.0356,  0.0250, -0.0346, -0.0577, -0.0593,  0.0211,\n",
       "                       -0.1042,  0.0132, -0.0609,  0.0837,  0.0626, -0.0660,  0.0940,  0.0581,\n",
       "                        0.1047, -0.0058, -0.0603,  0.0839,  0.0629, -0.1077, -0.1018,  0.0929,\n",
       "                        0.0484, -0.0596,  0.0198, -0.0060, -0.0218, -0.0611,  0.0076, -0.0053,\n",
       "                       -0.0194,  0.0815,  0.0766,  0.0153,  0.0565,  0.0910, -0.0097, -0.0865,\n",
       "                        0.0657, -0.0379,  0.1054,  0.0325,  0.0492, -0.0209, -0.0179,  0.0047,\n",
       "                       -0.0835, -0.0519, -0.0655, -0.0035],\n",
       "                      [-0.0280,  0.0676,  0.0876,  0.0939, -0.0645, -0.0424,  0.0671, -0.0027,\n",
       "                       -0.1007,  0.0673, -0.0554, -0.0860, -0.0202, -0.0206, -0.0189,  0.0451,\n",
       "                       -0.0491,  0.0639,  0.0294, -0.0415, -0.0725, -0.0120, -0.0429,  0.0663,\n",
       "                       -0.0182,  0.0312,  0.0402,  0.0498, -0.0683,  0.0860,  0.0757, -0.0824,\n",
       "                        0.0685,  0.0490, -0.0207, -0.0129,  0.0441, -0.0791,  0.0829, -0.0855,\n",
       "                        0.1072, -0.0429,  0.0463,  0.0302,  0.0469,  0.0232,  0.0662, -0.0695,\n",
       "                       -0.0668,  0.0244, -0.0844, -0.0905,  0.0868, -0.0917, -0.1033, -0.0700,\n",
       "                        0.0857,  0.0872,  0.0753,  0.0393,  0.0954, -0.0326, -0.0113,  0.0336,\n",
       "                       -0.0671, -0.0692, -0.0394, -0.0263, -0.0632,  0.0667,  0.0633,  0.1010,\n",
       "                       -0.0084, -0.0702,  0.0235,  0.0517,  0.0183,  0.0240, -0.0735, -0.0654,\n",
       "                        0.0509,  0.0845,  0.0287,  0.0091],\n",
       "                      [ 0.0905,  0.0937,  0.0108,  0.0168,  0.0081, -0.1084,  0.0603,  0.0031,\n",
       "                       -0.0107,  0.0284, -0.0845,  0.0011,  0.0225,  0.0334, -0.1016,  0.0532,\n",
       "                       -0.0023, -0.1012,  0.0080, -0.0201,  0.0324,  0.1005, -0.0046,  0.0237,\n",
       "                       -0.0067,  0.1082, -0.0607, -0.0291, -0.0466,  0.0105,  0.0230, -0.1082,\n",
       "                        0.0655, -0.0714,  0.1071,  0.0524, -0.0683,  0.0455,  0.1078, -0.0820,\n",
       "                       -0.0032,  0.0688, -0.0195, -0.0300, -0.0645, -0.0104,  0.0190, -0.0426,\n",
       "                       -0.0818,  0.0100,  0.0461, -0.0071,  0.0032,  0.1040, -0.0309, -0.0188,\n",
       "                        0.0575, -0.0947, -0.0081, -0.0461, -0.0036,  0.0987, -0.0266,  0.0290,\n",
       "                        0.0877, -0.0173, -0.0661,  0.0941, -0.0309, -0.0272, -0.0545,  0.0686,\n",
       "                        0.0370,  0.1062,  0.0887,  0.0851,  0.0517,  0.0560,  0.0457,  0.0626,\n",
       "                       -0.0841, -0.0762,  0.0984,  0.0582],\n",
       "                      [-0.1043, -0.0002, -0.0168, -0.0437,  0.0396,  0.1021, -0.0689,  0.0891,\n",
       "                        0.0386,  0.0122, -0.0731, -0.0820,  0.0302,  0.0746, -0.0273,  0.0402,\n",
       "                        0.0762,  0.0006,  0.0494, -0.0446,  0.0120, -0.0767, -0.0756,  0.0245,\n",
       "                       -0.0306, -0.0953,  0.0106,  0.0898,  0.0592, -0.0878,  0.1057, -0.0503,\n",
       "                       -0.0543,  0.0840, -0.0600, -0.0284,  0.0507, -0.0501,  0.0308,  0.0047,\n",
       "                       -0.0680, -0.0399,  0.0736, -0.0296,  0.0093, -0.0210,  0.0354,  0.0296,\n",
       "                       -0.0408,  0.0100, -0.1030,  0.0934, -0.0114, -0.0494,  0.0089, -0.0484,\n",
       "                        0.0747, -0.0480,  0.1085, -0.0029,  0.0161,  0.0952,  0.0505, -0.0040,\n",
       "                        0.0824, -0.0438,  0.1040,  0.0283,  0.0752,  0.0082,  0.0940,  0.0629,\n",
       "                        0.0217,  0.0695,  0.0885,  0.0093,  0.0680,  0.0109,  0.0368,  0.0093,\n",
       "                        0.0887, -0.0286, -0.0864,  0.0070],\n",
       "                      [-0.0065, -0.0704, -0.1032, -0.0713, -0.0777, -0.0832,  0.0140,  0.0754,\n",
       "                       -0.0941, -0.0773, -0.0384, -0.1061, -0.0269,  0.1064,  0.0336,  0.0344,\n",
       "                       -0.0054, -0.0986, -0.0755, -0.0807,  0.0314, -0.0892,  0.0584, -0.0509,\n",
       "                        0.0305,  0.0735, -0.0843,  0.0028, -0.1031,  0.0243,  0.0949,  0.0521,\n",
       "                        0.0452,  0.0072,  0.0566, -0.0718,  0.0163, -0.0356, -0.0658,  0.0489,\n",
       "                       -0.0837,  0.0242, -0.0302,  0.0836, -0.0104, -0.0517,  0.0053, -0.1074,\n",
       "                       -0.0509,  0.0804, -0.0188,  0.0894,  0.0758, -0.0246,  0.0373,  0.0601,\n",
       "                       -0.0665,  0.0152, -0.0892,  0.0628, -0.0799,  0.0237, -0.1039,  0.0554,\n",
       "                        0.0189, -0.0584,  0.0554,  0.0705,  0.0619, -0.1056, -0.0407, -0.0893,\n",
       "                       -0.0233, -0.1022,  0.0357,  0.0817,  0.0346, -0.0561,  0.1052,  0.0833,\n",
       "                       -0.0555,  0.0880, -0.0911,  0.0909],\n",
       "                      [-0.0480,  0.0106,  0.0587, -0.0454,  0.0674,  0.1001, -0.0574,  0.0386,\n",
       "                        0.0154, -0.0559, -0.0048, -0.0049,  0.0733, -0.0114,  0.0065, -0.0173,\n",
       "                        0.0820,  0.0887, -0.0628, -0.0617,  0.0336,  0.0004,  0.0815,  0.0538,\n",
       "                        0.0221, -0.0423,  0.0483, -0.0887, -0.0443,  0.0832,  0.0734,  0.0057,\n",
       "                       -0.1072,  0.0739, -0.0947,  0.0029,  0.0815, -0.0060,  0.1004,  0.0627,\n",
       "                       -0.0341, -0.0901, -0.0319, -0.0806, -0.0504, -0.0282, -0.0312, -0.0853,\n",
       "                        0.0309,  0.0819,  0.0723,  0.0354, -0.0817,  0.0079,  0.0058,  0.1010,\n",
       "                        0.0807, -0.1002, -0.0584, -0.0536, -0.0487,  0.0242,  0.0474,  0.0621,\n",
       "                        0.0094, -0.0362,  0.0731,  0.0144, -0.0656,  0.0637,  0.0087,  0.0900,\n",
       "                       -0.0277, -0.0620, -0.0692, -0.0095,  0.0409, -0.0507, -0.0681, -0.0744,\n",
       "                        0.0387, -0.0646,  0.0236, -0.0724],\n",
       "                      [-0.0398,  0.0844,  0.0665,  0.0898, -0.0585, -0.0135,  0.0983,  0.0740,\n",
       "                       -0.1080, -0.0788, -0.1044,  0.0864, -0.1072, -0.0565,  0.0668,  0.1081,\n",
       "                       -0.0543, -0.0319, -0.0389,  0.0217,  0.0207,  0.0189, -0.0973,  0.0963,\n",
       "                       -0.0003, -0.0184,  0.0212,  0.0287,  0.0091, -0.0706, -0.0429,  0.0361,\n",
       "                       -0.0282,  0.0652,  0.0136,  0.0249,  0.0898,  0.0015,  0.0698,  0.0528,\n",
       "                       -0.0461, -0.0051,  0.0132, -0.0843,  0.0398, -0.0818, -0.0096,  0.0536,\n",
       "                       -0.0416,  0.0378, -0.0121,  0.0853, -0.0497, -0.0487,  0.0709,  0.0972,\n",
       "                       -0.0984, -0.0882,  0.0729,  0.1006,  0.0399, -0.0147,  0.0422,  0.0514,\n",
       "                        0.0089,  0.0262, -0.0751, -0.0490, -0.0794,  0.0954,  0.0597,  0.0470,\n",
       "                        0.0811,  0.0950,  0.0897, -0.0457, -0.0248, -0.0478, -0.0064, -0.0646,\n",
       "                        0.0319,  0.0046, -0.0342,  0.0276],\n",
       "                      [-0.0849, -0.0527,  0.1074,  0.0987,  0.0909, -0.0339, -0.0733,  0.0262,\n",
       "                       -0.0933,  0.0231, -0.0606, -0.0787,  0.0029,  0.0944,  0.0761, -0.0035,\n",
       "                        0.0946,  0.0876,  0.0096,  0.0461,  0.0428, -0.0392, -0.1070, -0.0784,\n",
       "                        0.0087, -0.0242, -0.0794, -0.1029,  0.0718, -0.0823,  0.0814,  0.0766,\n",
       "                       -0.0399,  0.0002,  0.0748,  0.0749,  0.0379,  0.0527,  0.1059,  0.0327,\n",
       "                        0.0367, -0.0909, -0.0466, -0.0003, -0.0936,  0.0736,  0.0387, -0.0547,\n",
       "                        0.0856, -0.0419,  0.0589,  0.0808,  0.0786,  0.0080, -0.0545,  0.0268,\n",
       "                        0.1060, -0.0986, -0.0343,  0.0479, -0.0248,  0.0060, -0.0428, -0.0953,\n",
       "                       -0.0063, -0.0064, -0.0202, -0.0454, -0.1049,  0.0423,  0.0781, -0.0935,\n",
       "                        0.1017, -0.0640, -0.0018,  0.0532,  0.0822, -0.0023, -0.0505,  0.0891,\n",
       "                        0.0494,  0.0279,  0.0283,  0.0851]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.0477,  0.0474,  0.0233,  0.0299, -0.0092, -0.0711,  0.0982,  0.0256,\n",
       "                      -0.0035, -0.0730]))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict\n",
    "\n",
    "\"\"\"\n",
    "A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor\n",
    "Note that only layers with learnable parameters (convolutional layers, linear layers, etc.)\n",
    "and registered buffers (batchnorm’s running_mean) have entries in the model’s state_dict\n",
    "\n",
    "Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizer’s state,\n",
    "as well as the hyperparameters used\n",
    "\"\"\"\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "torch.save(net.state_dict(), 'torch-net.state')\n",
    "\n",
    "# this should load the entire weights for each layer, and equivalent tensor\n",
    "# then my defined tensor to simple matrix should transfer the weights.\n",
    "# then matrix multiplication HE operation.\n",
    "torch.load('torch-net.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
