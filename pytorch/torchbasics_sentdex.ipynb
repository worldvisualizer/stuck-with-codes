{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "# tensor: multidimensional array\n",
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([2,1])\n",
    "\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros([2, 5])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3589, 0.0179, 0.6491, 0.1870],\n",
       "        [0.2482, 0.4276, 0.2901, 0.4796],\n",
       "        [0.0742, 0.4969, 0.7600, 0.2533]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand([3,4])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3589, 0.0179, 0.6491, 0.1870, 0.2482, 0.4276],\n",
       "        [0.2901, 0.4796, 0.0742, 0.4969, 0.7600, 0.2533]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a reshape-equivalent for numpy, tensorflow\n",
    "y.view(2,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision # talking about computer vision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# gotta think about data formatting and fitting into the network\n",
    "# and batching as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dataset already available\n",
    "train = datasets.MNIST(\n",
    "    '', train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\n",
    "    '', train=False, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know the art of batching: improves performance, but takes long to converge\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 0, 4, 1, 0, 5, 8, 9, 2, 9])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break\n",
    "\n",
    "x,y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4824, 0.7176, 0.0157,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.9922, 0.9882, 0.2196,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882, 0.4941,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882, 0.4941,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3490, 0.9922, 0.9882, 0.1490,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8314, 1.0000, 0.8000, 0.0353,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1882, 0.9490, 0.9922, 0.6588, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2863, 0.9882, 0.9922, 0.2784, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.6627, 0.9882, 0.9922, 0.2471, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.6627, 0.9882, 0.8549, 0.0431, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0863, 0.9922, 0.9922, 0.5882, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0863, 0.9882, 0.9882, 0.4157, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0863, 0.9882, 0.9882, 0.0353, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0863, 0.9882, 0.9882, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0863, 0.9882, 0.9882, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2588, 0.9922, 0.9922, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0863, 0.9882, 0.9882, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.4667, 0.9882, 0.9882, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.3961, 0.9882, 0.7137, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0510, 0.8510, 0.2314, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(data[0][0].view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d9fed68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][0].view(28, 28)) # there's gonna be a problem with that 1.\n",
    "# for now, reshape it (flatten it) to 2d matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "# balancing the data: data label distribution can be an issue\n",
    "# checking the balance I guess\n",
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "for data in trainset:\n",
    "    xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "print(counter_dict)\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i] / total * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helping with autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTNet(\n",
      "  (fully_connected_1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fully_connected_2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fully_connected_3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fully_connected_4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    # defining feed-forward neural net. data flows forward from layer to layer to the output\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # where does that number come from?\n",
    "        # 28 * 28: size of the image. imagine it being flattened\n",
    "        # input is the raw image\n",
    "        # output is the neurons \n",
    "        self.fully_connected_1 = nn.Linear(28 * 28, 64)\n",
    "        \n",
    "        # those layers are hidden\n",
    "        self.fully_connected_2 = nn.Linear(64, 64)\n",
    "        self.fully_connected_3 = nn.Linear(64, 64)\n",
    "        # 10 classes as an output\n",
    "        self.fully_connected_4 = nn.Linear(64, 10)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # needs activation function for the data to scale\n",
    "        # you can set different activation function for different neurons, but that's another story.\n",
    "        x = functional.relu(self.fully_connected_1(x))\n",
    "        x = functional.relu(self.fully_connected_2(x))\n",
    "        x = functional.relu(self.fully_connected_3(x))\n",
    "        x = self.fully_connected_4(x)\n",
    "\n",
    "        # like axis: in which way do we want to sum the data to 1?\n",
    "        return functional.softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "net = MNISTNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0981, 0.1090, 0.0932, 0.1072, 0.1099, 0.1047, 0.0845, 0.0929, 0.1000,\n",
       "         0.1005]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1 is basically saying view function is being prepared for any shape...? \n",
    "X = torch.rand([28, 28]).view([-1, 28 * 28])\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9998, grad_fn=<NllLossBackward>)\n",
      "tensor(-1., grad_fn=<NllLossBackward>)\n",
      "tensor(-1., grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# just the right learning rate to converge to global minimum\n",
    "# in real world problems there's also a notion of decaying learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset: # from torch.utils.data.DataLoader\n",
    "        # batch of featuresets and labels\n",
    "        X, y = data\n",
    "        # https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/2\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        # scalar function (not a one-hot vector) requires loss function different from least-squares, etc\n",
    "        loss = functional.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9411, 10000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# why is there a with decorator with this function?\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "\n",
    "# why the fuck is the accuracy so high?\n",
    "correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
